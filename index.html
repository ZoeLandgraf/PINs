<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="PINs">
    <meta name="author" content="Zoe Landgraf">

    <title>Progressive Implicit Networks for Multi-Scale Neural Representations</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid" style="background-color:#fbd3ba">
    <div class="container"></div>
    <h2>PINs <br> </h2>
    <h3>Progressive Implicit Networks for Multi-Scale Neural Representations</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a> Zoe Landgraf, Alexander Sorkine-Hornung, Ricardo Silveira-Cabral</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2202.04713">Paper</a>
<!--        <a class="btn btn-primary" href="https://youtu.be/6IruTME-EXM>Video">Video</a>-->
    </div>
</div>

<div class="container">
    <div class="section">
       <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <video width="90%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="data/PINs_training_video.mp4" type="video/mp4">
                </video>
            </div>

        </div>
        <hr>
        <p>
            Multi-layer perceptrons (MLP) have proven to be effective scene encoders when combined with higher-dimensional
            projections of the input, commonly referred to as <i>positional encoding</i>.
            However, scenes with a wide frequency spectrum remain a challenge: choosing high frequencies
            for positional encoding introduces noise in low structure areas, while low frequencies result
            in poor fitting of detailed regions. To address this, we propose a progressive positional
            encoding, exposing a hierarchical MLP structure to incremental sets of frequency encodings.
            Our model accurately reconstructs scenes with wide frequency bands and learns a scene representation at progressive level
            of detail <i>without explicit per-level supervision</i>. The architecture is modular: each level
            encodes a continuous implicit representation that can be leveraged separately for its respective resolution,
            meaning a smaller network for coarser reconstructions.
            Experiments on several 2D and 3D datasets show improvements in reconstruction accuracy,
            representational capacity and training speed compared to baselines.
        </p>

    </div>


<!--     <div class="section">-->
<!--        <h2>Towards fast object-level scene understanding with approximate reasoning for occluded regions</h2>-->
<!--           <hr>-->
<!--        <p>-->

<!--&lt;!&ndash;            The classical approach to generate 3D shapes from depth images involves taking images from all sides of an&ndash;&gt;-->
<!--&lt;!&ndash;            object and fusing the re-projected points into a common 3D representation such as&ndash;&gt;-->
<!--&lt;!&ndash;            a truncated signed distance function (TSDF). This requires comprehensive scanning and still won't guarantee&ndash;&gt;-->
<!--&lt;!&ndash;            that all required viewpoints to generate a watertight surface reconstruction can be reached. 3D shape reconstruction&ndash;&gt;-->
<!--&lt;!&ndash;            becomes particularly challenging for composite scenes in which objects occlude each other.&ndash;&gt;-->
<!--&lt;!&ndash;            <br>&ndash;&gt;-->
<!--            Shape reconstruction is important for scene understanding, estimating free space and can be useful for tracking,-->
<!--            but in order to allow for interactive tasks such as object manipulation, an object-level understanding of-->
<!--            the scene is crucial; this requires instance segmentation. <i><b>With SIMstack we propose the first approach to-->
<!--            provide both 3D shape and instance decomposition for multiple (convex) stacked objects from a-->
<!--            single depth view. </b></i> By providing class-agnostic-->
<!--            instance segmentation and training on scenes composed of primitive shapes (SuperQuadrics),-->
<!--            our approach is designed to work for unknown objects.-->
<!--            <br><br>-->
<!--            Our model reconstructs and segments the visible regions with high fidelity while generating shape and instance-->
<!--            proposals for occluded regions, conditioned on the visible information obtained from depth.-->
<!--            This allows for fast reconstruction and decomposition in the visible area and fast approximate object-level-->
<!--            reasoning for occluded areas.-->

<!--            <br><br>-->
<!--            SIMstack provides fast 3D shape and instance decomposition into convex primitives,-->
<!--            which could be used for downstream applications (e.g to initialise a multi-view scanning system-->
<!--            to capture more detail) and allows for fast multi-object reasoning,-->
<!--            useful for interactive tasks such as the precise (non-disruptive) grasping we demonstrate.-->
<!--            Our method can leverage multi-view information for improved reconstruction which makes it by design a-->
<!--            candidate for incremental reconstruction systems.-->

<!--        </p>-->
<!--    </div>-->

    <div class="section">
        <h2>Method</h2>
         <hr>
        <p>
              We formulate the task of reconstructing a scene <i>S</i> as a composition of a
              base component <i>c</i> and a set of residuals <i>R</i>. Each residual is parameterised by an MLP and the final
              reconstruction is produced by a simple addition of all residuals and <i>c</i>.
              Intermediate levels of detail
              can be obtained by combining a subset of the residuals (e.g. to obtain the first level of detail, the first
              residual is added to the base component <i>c</i>).

            <br><br>
            Our architecture is composed of multiple stacked small MLPs <i>M</i>,
            each receiving as input a set of Fourier Feature mappings <i>f</i>
            and the output of the previous level. The first level receives as input the raw coordinates <b>x</b>.

        </p>

         <div class="row align-items-center">

             <img src="data/Architecture_PINs.png" style="width:100%">
         </div>


        <br><br>
        Being exposed to low frequencies, early layers learn a coarse representation of the scene.
        Later layers that are exposed to higher frequencies,
        only focus on adding missing detail.
         <div class="row align-items-center">

             <img src="data/Residual_learn_incremental_detail.png" style="width:100%">
         </div>


<!--        Sampling from the VAE's latent space generates a variety of 3D shape and instance decomposition proposals-->
<!--        in the scene's occluded region, while keeping the 3D shape in the visible region constant.-->

<!--        <div class="row align-items-center">-->
<!--           <div class="col justify-content-center text-center">-->
<!--                <video width="90%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                    <source src="img/sampling_from_latent_space_example_cropped.mp4" type="video/mp4">-->
<!--                </video>-->
<!--            </div>-->

<!--        </div>-->

<!--    </div>-->

    <div class="section">
        <h2>Results</h2>
        <hr>
        <p> <b>2D regression</b> Compared to our baselines <a href="https://arxiv.org/abs/2006.10739">Fourier Feature Networks</a> (FF Nets),
           <a href="https://arxiv.org/abs/2006.09661">SIREN</a> and <a href="https://arxiv.org/abs/2104.09125">SAPE</a>,
            we can reconstruct images with wide frequency bands (images that contain fine detail as well as homogeneous regions)
            with reduced or no noise in smooth regions.
        </p>


        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/2D_qual_comparison_COCO_crop.png" style="width:100%">
            </div>

        </div>
        <hr>
        <p> <b>3D regression</b> Our method can also reconstruct 3D scenes at progressive level of detail at a higher degree of accuracy
            compared to our baselines</p>

        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/3D_qual_results_chair.png" style="width:100%">
            </div>

        </div>

<!--            <div class="col justify-content-center text-center">-->
<!--                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                    <source src="img/circle_synthetic_scene_cropped.mp4" type="video/mp4">-->
<!--                </video>-->
<!--            </div>-->

<!--        </div>-->
        <hr>
        <p> When presented with <b>limited number of training samples</b>, PINs can recover the scene more accurately
        </p>

        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/Reconstructing_with_low_training_samples.png" style="width:100%">
            </div>

        </div>


    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{landgraf2022PINs,
                author = {Landgraf, Zoe
                          and Sorkine-Horning Alexander
                          and Silveira-Cabral Ricardo
                          },
                title = {PINs: Progressive Implicit Networks for Multi-Scale Neural Representations},
                booktitle = {ICML},
                year={2022}
            }
        </div>
    </div>

<!--    <hr>-->

<!--    <footer>-->
<!--        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Zoe Landgraf</a></p>-->
<!--    </footer>-->


</div>

    </body>
</html>