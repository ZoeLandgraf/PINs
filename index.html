<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="PINs">
    <meta name="author" content="Zoe Landgraf">

    <title>Progressive Implicit Networks for Multi-Scale Neural Representations</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid" style="background-color:#fbd3ba">
    <div class="container"></div>
    <h2>PINs <br> </h2>
    <h3>Progressive Implicit Networks for Multi-Scale Neural Representations</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a> Zoe Landgraf<sup> +,* </sup>, Alexander Sorkine-Hornung<sup>+</sup>, Ricardo Silveira-Cabral<sup>+</sup></a>
        <br><br>
        Imperial College London<sup>*</sup>, Meta<sup>+</sup>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2202.04713">Paper</a>
<!--        <a class="btn btn-primary" href="https://youtu.be/6IruTME-EXM>Video">Video</a>-->
    </div>
</div>

<div class="container">
    <div class="section">
       <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <video width="90%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="data/PINs_training_video.mp4" type="video/mp4">
                </video>
            </div>

        </div>
        <hr>
        <p>
            Multi-layer perceptrons (MLP) have proven to be effective scene encoders when combined with higher-dimensional
            projections of the input, commonly referred to as <i>positional encoding</i>.
            However, scenes with a wide frequency spectrum remain a challenge: choosing high frequencies
            for positional encoding introduces noise in low structure areas, while low frequencies result
            in poor fitting of detailed regions. To address this, we propose a progressive positional
            encoding, exposing a hierarchical MLP structure to incremental sets of frequency encodings.
            Our model accurately reconstructs scenes with wide frequency bands and learns a scene representation at progressive level
            of detail <i>without explicit per-level supervision</i>. The architecture is modular: each level
            encodes a continuous implicit representation that can be leveraged separately for its respective resolution,
            meaning a smaller network for coarser reconstructions.
            Experiments on several 2D and 3D datasets show improvements in reconstruction accuracy,
            representational capacity and training speed compared to baselines.
        </p>

    </div>


    <div class="section">
        <h2>Method</h2>
         <hr>
        <p>
              We formulate the task of reconstructing a scene <i>S</i> as a composition of a
              base component <i>c</i> and a set of residuals <i>R<sub>1 ... N</sub></i>. Each residual is parameterised by an MLP and the final
              reconstruction is produced by a simple addition of all residuals and <i>c</i>.
              Intermediate levels of detail
              can be obtained by combining a subset of the residuals (e.g. to obtain the first level of detail <i>S<sub>1</sub></i>, the first
              residual is added to the base component <i>c</i>).

             <br><br>
            <b>Progressive Fourier Feature encoding</b> As positional encoding, we use a Fourier Feature mapping
            (see <a href="https://arxiv.org/abs/2006.10739">Fourier Feature Networks</a>) and sort the encoding frequencies
            in ascending order.

            <br><br>
            <b>Network architecture</b> Our model is composed of multiple stacked small MLPs <i>M</i>,
            each receiving as input a subset of our progressive Fourier Feature encoding <i>f<sub>1 ... N</sub></i>
            and the output of the previous level. The first level receives as input the raw coordinates <b>x</b>.

        </p>

         <div class="row align-items-center">

             <img src="data/Architecture_PINs.png" style="width:100%">
         </div>


        <br><br>
        Being exposed to low frequencies, early layers learn a coarse representation of the scene.
        Later layers that are exposed to higher frequencies,
        only focus on adding missing detail.
         <div class="row align-items-center">

             <img src="data/Residual_learn_incremental_detail.png" style="width:100%">
         </div>


<!--        Sampling from the VAE's latent space generates a variety of 3D shape and instance decomposition proposals-->
<!--        in the scene's occluded region, while keeping the 3D shape in the visible region constant.-->

<!--        <div class="row align-items-center">-->
<!--           <div class="col justify-content-center text-center">-->
<!--                <video width="90%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                    <source src="img/sampling_from_latent_space_example_cropped.mp4" type="video/mp4">-->
<!--                </video>-->
<!--            </div>-->

<!--        </div>-->

<!--    </div>-->

    <div class="section">
        <h2>Results</h2>
        <hr>
        <p> <b>2D regression</b> Compared to our baselines <a href="https://arxiv.org/abs/2006.10739">Fourier Feature Networks</a> (FF Nets),
           <a href="https://arxiv.org/abs/2006.09661">SIREN</a> and <a href="https://arxiv.org/abs/2104.09125">SAPE</a>,
            we can reconstruct images with wide frequency bands (images that contain fine detail as well as homogeneous regions)
            with reduced or no noise in smooth regions.
        </p>


        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/2D_qual_comparison_COCO_crop.png" style="width:100%">
            </div>

        </div>
        <hr>
        <p> <b>3D regression</b> Our method can also reconstruct 3D scenes at progressive level of detail at a higher degree of accuracy
            compared to our baselines: </p>

        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/3D_qual_results_chair.png" style="width:100%">
            </div>

        </div>

<!--            <div class="col justify-content-center text-center">-->
<!--                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">-->
<!--                    <source src="img/circle_synthetic_scene_cropped.mp4" type="video/mp4">-->
<!--                </video>-->
<!--            </div>-->

<!--        </div>-->
        <hr>
        <p> When presented with <b>limited number of training samples</b>, PINs can recover the scene more accurately
            compared to our baselines:
        </p>

        <div class="row align-items-center">
           <div class="col justify-content-center text-center">
                <img src="data/Reconstructing_with_low_training_samples.png" style="width:100%">
            </div>

        </div>


    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{landgraf2022PINs,
                author = {Landgraf, Zoe
                          and Sorkine-Horning Alexander
                          and Silveira-Cabral Ricardo
                          },
                title = {PINs: Progressive Implicit Networks for Multi-Scale Neural Representations},
                booktitle = {ICML},
                year={2022}
            }
        </div>
    </div>

<!--    <hr>-->

<!--    <footer>-->
<!--        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Zoe Landgraf</a></p>-->
<!--    </footer>-->


</div>

    </body>
</html>